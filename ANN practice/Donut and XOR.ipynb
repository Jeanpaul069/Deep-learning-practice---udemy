{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 ll: -935.497242817 classification rate: 0.512\n",
      "i: 1000 ll: -405.457003923 classification rate: 0.917\n",
      "i: 2000 ll: -234.302419597 classification rate: 0.981\n",
      "i: 3000 ll: -169.882325424 classification rate: 0.982\n",
      "i: 4000 ll: -136.092914476 classification rate: 0.985\n",
      "i: 5000 ll: -115.454925855 classification rate: 0.989\n",
      "i: 6000 ll: -101.437040921 classification rate: 0.991\n",
      "i: 7000 ll: -91.1352341845 classification rate: 0.992\n",
      "i: 8000 ll: -83.0926715761 classification rate: 0.992\n",
      "i: 9000 ll: -76.6905901974 classification rate: 0.993\n",
      "i: 10000 ll: -71.4830974928 classification rate: 0.993\n",
      "i: 11000 ll: -67.1282024286 classification rate: 0.993\n",
      "i: 12000 ll: -63.4200389503 classification rate: 0.993\n",
      "i: 13000 ll: -60.278190034 classification rate: 0.993\n",
      "i: 14000 ll: -57.5975347491 classification rate: 0.996\n",
      "i: 15000 ll: -55.2647649844 classification rate: 0.996\n",
      "i: 16000 ll: -53.2063991571 classification rate: 0.995\n",
      "i: 17000 ll: -51.3722022365 classification rate: 0.995\n",
      "i: 18000 ll: -49.7263958083 classification rate: 0.995\n",
      "i: 19000 ll: -48.2441688409 classification rate: 0.996\n",
      "i: 20000 ll: -46.9083270276 classification rate: 0.996\n",
      "i: 21000 ll: -45.7049718745 classification rate: 0.996\n",
      "i: 22000 ll: -44.6201725628 classification rate: 0.996\n",
      "i: 23000 ll: -43.6394217931 classification rate: 0.996\n",
      "i: 24000 ll: -42.7488681409 classification rate: 0.996\n",
      "i: 25000 ll: -41.9364091951 classification rate: 0.996\n",
      "i: 26000 ll: -41.1919689652 classification rate: 0.996\n",
      "i: 27000 ll: -40.5072542023 classification rate: 0.996\n",
      "i: 28000 ll: -39.8753804981 classification rate: 0.997\n",
      "i: 29000 ll: -39.2905509474 classification rate: 0.997\n",
      "i: 30000 ll: -38.7478214056 classification rate: 0.997\n",
      "i: 31000 ll: -38.2429347573 classification rate: 0.997\n",
      "i: 32000 ll: -37.7722012188 classification rate: 0.998\n",
      "i: 33000 ll: -37.332408137 classification rate: 0.998\n",
      "i: 34000 ll: -36.9207490188 classification rate: 0.998\n",
      "i: 35000 ll: -36.5347655177 classification rate: 0.998\n",
      "i: 36000 ll: -36.1722984424 classification rate: 0.998\n",
      "i: 37000 ll: -35.8314453297 classification rate: 0.998\n",
      "i: 38000 ll: -35.510523188 classification rate: 0.997\n",
      "i: 39000 ll: -35.2080357707 classification rate: 0.997\n",
      "i: 40000 ll: -34.9226451689 classification rate: 0.997\n",
      "i: 41000 ll: -34.6531476364 classification rate: 0.997\n",
      "i: 42000 ll: -34.3984534681 classification rate: 0.997\n",
      "i: 43000 ll: -34.1575705779 classification rate: 0.997\n",
      "i: 44000 ll: -33.9295912768 classification rate: 0.997\n",
      "i: 45000 ll: -33.7136816859 classification rate: 0.997\n",
      "i: 46000 ll: -33.5090732303 classification rate: 0.997\n",
      "i: 47000 ll: -33.3150557274 classification rate: 0.997\n",
      "i: 48000 ll: -33.1309716754 classification rate: 0.997\n",
      "i: 49000 ll: -32.9562114358 classification rate: 0.997\n",
      "i: 50000 ll: -32.7902090885 classification rate: 0.997\n",
      "i: 51000 ll: -32.6324388 classification rate: 0.997\n",
      "i: 52000 ll: -32.4824115912 classification rate: 0.997\n",
      "i: 53000 ll: -32.3396724293 classification rate: 0.997\n",
      "i: 54000 ll: -32.2037975886 classification rate: 0.997\n",
      "i: 55000 ll: -32.0743922421 classification rate: 0.996\n",
      "i: 56000 ll: -31.951088255 classification rate: 0.996\n",
      "i: 57000 ll: -31.8335421599 classification rate: 0.996\n",
      "i: 58000 ll: -31.7214332954 classification rate: 0.996\n",
      "i: 59000 ll: -31.614462094 classification rate: 0.996\n",
      "i: 60000 ll: -31.5123485074 classification rate: 0.996\n",
      "i: 61000 ll: -31.4148305575 classification rate: 0.996\n",
      "i: 62000 ll: -31.3216630032 classification rate: 0.996\n",
      "i: 63000 ll: -31.232616114 classification rate: 0.996\n",
      "i: 64000 ll: -31.1474745414 classification rate: 0.996\n",
      "i: 65000 ll: -31.0660362803 classification rate: 0.996\n",
      "i: 66000 ll: -30.9881117137 classification rate: 0.996\n",
      "i: 67000 ll: -30.9135227322 classification rate: 0.996\n",
      "i: 68000 ll: -30.8421019242 classification rate: 0.997\n",
      "i: 69000 ll: -30.7736918295 classification rate: 0.997\n",
      "i: 70000 ll: -30.7081442509 classification rate: 0.997\n",
      "i: 71000 ll: -30.64531962 classification rate: 0.997\n",
      "i: 72000 ll: -30.5850864118 classification rate: 0.997\n",
      "i: 73000 ll: -30.5273206044 classification rate: 0.997\n",
      "i: 74000 ll: -30.4719051795 classification rate: 0.997\n",
      "i: 75000 ll: -30.4187296608 classification rate: 0.997\n",
      "i: 76000 ll: -30.3676896876 classification rate: 0.997\n",
      "i: 77000 ll: -30.3186866189 classification rate: 0.997\n",
      "i: 78000 ll: -30.271627168 classification rate: 0.997\n",
      "i: 79000 ll: -30.2264230627 classification rate: 0.997\n",
      "i: 80000 ll: -30.1829907308 classification rate: 0.997\n",
      "i: 81000 ll: -30.1412510078 classification rate: 0.997\n",
      "i: 82000 ll: -30.1011288655 classification rate: 0.997\n",
      "i: 83000 ll: -30.0625531596 classification rate: 0.997\n",
      "i: 84000 ll: -30.0254563954 classification rate: 0.997\n",
      "i: 85000 ll: -29.9897745087 classification rate: 0.997\n",
      "i: 86000 ll: -29.9554466627 classification rate: 0.997\n",
      "i: 87000 ll: -29.9224150582 classification rate: 0.997\n",
      "i: 88000 ll: -29.8906247569 classification rate: 0.997\n",
      "i: 89000 ll: -29.8600235157 classification rate: 0.997\n",
      "i: 90000 ll: -29.830561633 classification rate: 0.997\n",
      "i: 91000 ll: -29.8021918043 classification rate: 0.997\n",
      "i: 92000 ll: -29.7748689876 classification rate: 0.997\n",
      "i: 93000 ll: -29.748550277 classification rate: 0.997\n",
      "i: 94000 ll: -29.723194785 classification rate: 0.997\n",
      "i: 95000 ll: -29.6987635315 classification rate: 0.997\n",
      "i: 96000 ll: -29.6752193406 classification rate: 0.997\n",
      "i: 97000 ll: -29.6525267427 classification rate: 0.997\n",
      "i: 98000 ll: -29.6306518841 classification rate: 0.997\n",
      "i: 99000 ll: -29.6095624407 classification rate: 0.997\n",
      "i: 100000 ll: -29.5892275378 classification rate: 0.997\n",
      "i: 101000 ll: -29.5696176742 classification rate: 0.997\n",
      "i: 102000 ll: -29.5507046518 classification rate: 0.997\n",
      "i: 103000 ll: -29.5324615077 classification rate: 0.997\n",
      "i: 104000 ll: -29.5148624521 classification rate: 0.997\n",
      "i: 105000 ll: -29.4978828083 classification rate: 0.997\n",
      "i: 106000 ll: -29.4814989574 classification rate: 0.997\n",
      "i: 107000 ll: -29.4656882851 classification rate: 0.997\n",
      "i: 108000 ll: -29.4504291326 classification rate: 0.997\n",
      "i: 109000 ll: -29.4357007489 classification rate: 0.997\n",
      "i: 110000 ll: -29.4214832476 classification rate: 0.997\n",
      "i: 111000 ll: -29.4077575638 classification rate: 0.997\n",
      "i: 112000 ll: -29.394505416 classification rate: 0.997\n",
      "i: 113000 ll: -29.3817092676 classification rate: 0.997\n",
      "i: 114000 ll: -29.3693522921 classification rate: 0.997\n",
      "i: 115000 ll: -29.3574183397 classification rate: 0.997\n",
      "i: 116000 ll: -29.3458919053 classification rate: 0.997\n",
      "i: 117000 ll: -29.3347580991 classification rate: 0.997\n",
      "i: 118000 ll: -29.3240026174 classification rate: 0.997\n",
      "i: 119000 ll: -29.3136117163 classification rate: 0.997\n",
      "i: 120000 ll: -29.3035721856 classification rate: 0.997\n",
      "i: 121000 ll: -29.2938713251 classification rate: 0.997\n",
      "i: 122000 ll: -29.2844969207 classification rate: 0.997\n",
      "i: 123000 ll: -29.2754372234 classification rate: 0.997\n",
      "i: 124000 ll: -29.2666809275 classification rate: 0.997\n",
      "i: 125000 ll: -29.2582171512 classification rate: 0.997\n",
      "i: 126000 ll: -29.2500354177 classification rate: 0.997\n",
      "i: 127000 ll: -29.2421256368 classification rate: 0.997\n",
      "i: 128000 ll: -29.2344780878 classification rate: 0.997\n",
      "i: 129000 ll: -29.2270834029 classification rate: 0.997\n",
      "i: 130000 ll: -29.2199325515 classification rate: 0.997\n",
      "i: 131000 ll: -29.2130168248 classification rate: 0.997\n",
      "i: 132000 ll: -29.2063278214 classification rate: 0.997\n",
      "i: 133000 ll: -29.1998574332 classification rate: 0.997\n",
      "i: 134000 ll: -29.1935978321 classification rate: 0.997\n",
      "i: 135000 ll: -29.1875414563 classification rate: 0.997\n",
      "i: 136000 ll: -29.1816809985 classification rate: 0.997\n",
      "i: 137000 ll: -29.1760093928 classification rate: 0.997\n",
      "i: 138000 ll: -29.1705198031 classification rate: 0.997\n",
      "i: 139000 ll: -29.1652056113 classification rate: 0.997\n",
      "i: 140000 ll: -29.1600604057 classification rate: 0.997\n",
      "i: 141000 ll: -29.1550779693 classification rate: 0.997\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z = np.tanh(X.dot(W1) + b1)\n",
    "    activation = Z.dot(W2) + b2\n",
    "    y = 1 / (1 + np.exp(-activation ) )\n",
    "    \n",
    "    return y, Z\n",
    "\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    Y, _ = forward(X, W1, b1, W2, b2)\n",
    "    return np.round(Y)\n",
    "    \n",
    "def derivative_w2(Z, T, Y):\n",
    "    return Z.T.dot(T-Y)\n",
    "\n",
    "def derivative_b2(T, Y):\n",
    "    return (T-Y).sum()\n",
    "\n",
    "def derivative_w1(X, Z, T, Y, W2):\n",
    "    dZ = np.outer(T-Y,W2) * (1 - Z * Z) # equal to (T-Y).dot(W2.T) * (1 - Z * Z)\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "def derivative_b1(Z, T, Y, W2):\n",
    "    # dZ = np.outer(T-Y, W2) * Z * (1 - Z) # this is for sigmoid activation\n",
    "    dZ = np.outer(T-Y, W2) * (1 - Z * Z) # this is for tanh activation\n",
    "    return dZ.sum(axis = 0)\n",
    "\n",
    "#becareful of this one\n",
    "def cost(T, pY):\n",
    "    return np.sum(T*np.log(pY) + (1-T)*np.log(1-pY))\n",
    "\n",
    "def test_xor():\n",
    "    X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "    Y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    W1 = np.random.randn(2,5)\n",
    "    b1 = np.zeros(5)\n",
    "    W2 = np.random.randn(5)\n",
    "    b2 = 0\n",
    "    LL = [] #likelihood\n",
    "    a = 10e-3 #learning rate\n",
    "    regularization = 0.\n",
    "    last_error_rate = None\n",
    "    for i in range(30000):\n",
    "        pY, Z = forward(X, W1, b1, W2, b2)\n",
    "        ll = cost(Y, pY)\n",
    "        \n",
    "        prediction = predict(X, W1, b1, W2, b2)\n",
    "        er = np.mean(prediction != Y)\n",
    "\n",
    "        if er != last_error_rate:\n",
    "            last_error_rate = er\n",
    "            print (\"error rate: \",er)\n",
    "            print (\"true: \", Y)\n",
    "            print (\"prediction:\", prediction)\n",
    "        if LL and ll < LL[-1]:\n",
    "            print (\"early exit\")\n",
    "            break\n",
    "        LL.append(ll)\n",
    "        W2 += a*(derivative_w2(Z, Y, pY) - regularization * W2)\n",
    "        b2 += a*(derivative_b2(Y, pY) - regularization * b2)\n",
    "        W1 += a*(derivative_w1(X, Z, Y, pY, W2) - regularization * W1)\n",
    "        b1 += a*(derivative_b1(Z, Y, pY, W2) - regularization * b1)\n",
    "        if i % 1000 == 0:\n",
    "            print (i, \"%0.*f\" % (10, ll))\n",
    "        \n",
    "    print (\"final class rate: \", np.mean(prediction == Y))\n",
    "    plt.plot(LL)\n",
    "    plt.show()\n",
    "\n",
    "def test_donut():\n",
    "    # donut example\n",
    "    N = 1000\n",
    "    R_inner = 5\n",
    "    R_outer = 10\n",
    "\n",
    "    # distance from origin is radius + random normal\n",
    "    # angle theta is uniformly distributed between (0, 2pi)\n",
    "    R1 = np.random.randn(int(N/2)) + R_inner\n",
    "    theta = 2*np.pi*np.random.random(int(N/2))\n",
    "    X_inner = np.concatenate([[R1 * np.cos(theta)], [R1 * np.sin(theta)]]).T\n",
    "\n",
    "    R2 = np.random.randn(int(N/2)) + R_outer\n",
    "    theta = 2*np.pi*np.random.random(int(N/2))\n",
    "    X_outer = np.concatenate([[R2 * np.cos(theta)], [R2 * np.sin(theta)]]).T\n",
    "\n",
    "    X = np.concatenate([ X_inner, X_outer ])\n",
    "    Y = np.array([0]*int(N/2) + [1]*int(N/2))\n",
    "\n",
    "    n_hidden = 8\n",
    "    W1 = np.random.randn(2, n_hidden)\n",
    "    b1 = np.random.randn(n_hidden)\n",
    "    W2 = np.random.randn(n_hidden)\n",
    "    b2 = np.random.randn(1)\n",
    "    LL = [] # keep track of likelihoods\n",
    "    learning_rate = 0.00005\n",
    "    regularization = 0.2\n",
    "    last_error_rate = None\n",
    "    for i in range(160000):\n",
    "        pY, Z = forward(X, W1, b1, W2, b2)\n",
    "        ll = cost(Y, pY)\n",
    "        prediction = predict(X, W1, b1, W2, b2)\n",
    "        er = np.abs(prediction - Y).mean()\n",
    "        LL.append(ll)\n",
    "        W2 += learning_rate * (derivative_w2(Z, Y, pY) - regularization * W2)\n",
    "        b2 += learning_rate * (derivative_b2(Y, pY) - regularization * b2)\n",
    "        W1 += learning_rate * (derivative_w1(X, Z, Y, pY, W2) - regularization * W1)\n",
    "        b1 += learning_rate * (derivative_b1(Z, Y, pY, W2) - regularization * b1)\n",
    "        if i % 1000 == 0:\n",
    "            print (\"i:\", i, \"ll:\", ll, \"classification rate:\", 1 - er)\n",
    "    plt.plot(LL)\n",
    "    plt.show()\n",
    "if __name__ == \"__main__\":\n",
    "    #test_xor()\n",
    "    test_donut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
